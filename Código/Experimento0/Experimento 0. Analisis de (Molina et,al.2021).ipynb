{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsBFD7icF6Xc"
   },
   "source": [
    "# Experimento 0 : Revisión del artículo \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\".\n",
    "\n",
    "En este experimiento usaremos la arquitectura y estructura de entrenamiento planteada en \"A prelimiminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\" para comprobar si se validan en un problema multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuarción del sistema para desplegar tensorflow en GPU. Es necesario tener instalado CUDA y cudNN\n",
    "import tensorflow as tf\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxC7JJwlFuvk"
   },
   "outputs": [],
   "source": [
    "import numpy as np  #Librería básica de manejo de vectores y tensores\n",
    "import pandas as pd #Librería básica para la gestión y análisis de datos\n",
    "\n",
    "from glob import glob \n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkqu4FPPGQgV"
   },
   "source": [
    "## Rutas y variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZazLFjjGZVf"
   },
   "outputs": [],
   "source": [
    "#Rutas de los datos.\n",
    " \n",
    "data_dir = os.path.dirname(os.path.realpath(\"../TFG/Datos/HAM10000_metadata.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.realpath(data_dir + \"/HAM10000_metadata.csv\")\n",
    "\n",
    "#Variables globales\n",
    "\n",
    "altura = 50\n",
    "longitud = 50\n",
    "clases = 7\n",
    "\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "print(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD9w48fEGfkF"
   },
   "source": [
    "## Creación del marco de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pf0IBVhhGehu",
    "outputId": "af59739a-413e-4a7d-c5ac-b775ab675e5d"
   },
   "outputs": [],
   "source": [
    "#Inicializando el dataFrame\n",
    "\n",
    "dataFrame=pd.read_csv(csv_path)\n",
    "\n",
    "#Mezclando carpetas.\n",
    "\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "\n",
    "# Inicializando diccionario de categorías\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "#Añadiendo columnas al dataFrame para que sea más legible.\n",
    "\n",
    "dataFrame['path'] = dataFrame['image_id'].map(imageid_path_dict.get)\n",
    "dataFrame['cell_type'] = dataFrame['dx'].map(lesion_type_dict.get) \n",
    "dataFrame['cell_type_idx'] = pd.Categorical(dataFrame['cell_type']).codes\n",
    "dataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1Py3CnE0bT",
    "outputId": "aea7fc1b-b1d9-42af-caa5-40293bbc2136"
   },
   "source": [
    "## Select_network\n",
    "\n",
    "Este método permite seleccionar la parte superior de una red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9ySeIuMHak_"
   },
   "outputs": [],
   "source": [
    "def select_network(nn_base_arch):\n",
    "    if nn_base_arch == 'HELP':\n",
    "        print(\"Se encuentra disponible:\\n\"+\n",
    "              \"- CNN_SOCO\")\n",
    "    \n",
    "    if nn_base_arch =='CNN_SOCO':\n",
    "        nn = cnn_soco()\n",
    "        \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los métodos que se describen acontinuación permitirán crear diferentes tipos de capas superiores que se usarán en el método select_network. Los métodos son : \n",
    "\n",
    " - cnn_soco = Es una réplica de la red usada en el estudio \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_soco():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1), activation='relu',input_shape=(altura,longitud,3)))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1),activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    " \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos que se describen a continuación generarán la capa de salida de nuestra red : \n",
    " - build : Modificación de la red_SOCO, añadimos una capa de Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(nn):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(nn)\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(128))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(clases,activation='softmax'))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se procede a crear un método que permita balancear la carga de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NO6Snfw-KYe4"
   },
   "outputs": [],
   "source": [
    "def balanced_dataset(df):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    #df = pd.DataFrame()\n",
    "    \n",
    "    for cat in df['cell_type_idx'].unique():\n",
    "        temp = resample(df[df['cell_type_idx'] == cat], \n",
    "                        replace=True,     # sample with replacement\n",
    "                        n_samples=2500,   # to match majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df_balanced = pd.concat([df_balanced, temp])\n",
    " \n",
    "    df_balanced['cell_type'].value_counts()\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def load_img_data(size, df, balanced=False):\n",
    "    \"\"\"\n",
    "        ..\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "    \n",
    "    img_h, img_w = size, size\n",
    "    imgs = []\n",
    "    \n",
    "    if balanced:\n",
    "        df = balanced_dataset(df)\n",
    "    \n",
    "    image_paths = list(df['path'])\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=0)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    #imgs = imgs.astype(np.float32) / 255.\n",
    "    \n",
    "    return imgs, df['cell_type_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LMBlVsJN-5o"
   },
   "outputs": [],
   "source": [
    "del dataFrame\n",
    "del imgs\n",
    "del target\n",
    "del x_train\n",
    "del x_test\n",
    "del y_train\n",
    "del y_test\n",
    "del x_val\n",
    "del y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos y generamos el set de datos,entrenamiento y validación para cada experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_general_data():\n",
    "    \n",
    "    imgs, target = load_img_data(altura, dataFrame, balanced=True)\n",
    "    \n",
    "    x_train, x_transferLearning, y_train, y_transferLearning = train_test_split(imgs, target, test_size=0.60)\n",
    "       \n",
    "    source_data = [ x_transferLearning , y_transferLearning ]\n",
    "    target_data = [ x_train , y_train ]\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(target_data[0], target_data[1], test_size=0.70)\n",
    "    \n",
    "    train_data = [x_train,y_train]\n",
    "    test_data = [x_test,y_test]\n",
    "    \n",
    "    return source_data,train_data,test_data\n",
    "\n",
    "#############################################################################################\n",
    "# Creamos varios métodos que nos permiten simular los procesos de entrenamiento del estudio #\n",
    "#############################################################################################\n",
    "\n",
    "def get_data_for_ex_1(source_data,train_data,test_data):\n",
    "    \n",
    "    x_train = train_data[0]\n",
    "    y_train = train_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_2(source_data,train_data,test_data):\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_3(source_data,train_data,test_data):\n",
    "    data_0 = source_data[0]\n",
    "    data_1 = source_data[1]\n",
    "    \n",
    "    for e in train_data[0]:\n",
    "        np.append(data_0,e)\n",
    "        \n",
    "    for e in train_data[1]:\n",
    "        np.append(data_1,e)\n",
    "        \n",
    "    x_train = data_0\n",
    "    y_train = data_1\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_4(source_data,train_data,test_data):\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_retrain = train_data[0]\n",
    "    y_retrain = train_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_retrain,x_test,y_train,y_retrain,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constantes del experimento\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "RMSpropEstudio = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corremos los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data,train_data,test_data = load_general_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('GPU:0'): # Esta linea permite utilizar el manejador de procesos de Python para gestionar el uso de la GPU\n",
    "    x_train,x_test,y_train,y_test = get_data_for_ex_1(source_data,train_data,test_data)\n",
    "    res1,evaluations1 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "    x_train,x_test,y_train,y_test = get_data_for_ex_2(source_data,train_data,test_data)\n",
    "    res2,evaluations2 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "    x_train,x_test,y_train,y_test = get_data_for_ex_3(source_data,train_data,test_data)\n",
    "    res3,evaluations3 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "    x_train,x_retrain,x_test,y_train,y_retrain,y_test = get_data_for_ex_4(source_data,train_data,test_data)\n",
    "    res4,res5,evaluations4,evaluations5 = run_experiment_4(nn_base_arch,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_for_ex1_w_test_score(res1,evaluations1,EPOCHS,\"Experiment 1\")\n",
    "plot_acc_for_ex1_w_test_score(res2,evaluations2,EPOCHS,\"Experiment 2\")\n",
    "plot_acc_for_ex1_w_test_score(res3,evaluations3,EPOCHS,\"Experiment 3\")\n",
    "plot_acc_for_ex1_w_test_score(res3,evaluations3,EPOCHS,\"Experiment 3\")\n",
    "plot_acc_all_experiments(res1,res2,res3,res5,evaluations1,evaluations2,evaluations3,evaluations5,EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos para correr los entrenamientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSpropEstudio, metrics=['accuracy','mse'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[cpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation\n",
    "\n",
    "def run_train_w_model(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSpropEstudio, metrics=['accuracy','mse'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[cpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation,model\n",
    "\n",
    "def re_train(model,epocas):\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,batch_size = BATCH_SIZE,verbose=0)\n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "    return history,evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSpropEstudio, metrics=['accuracy','mse'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[checkpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation\n",
    "\n",
    "nn_base_arch = \"CNN_SOCO\"\n",
    "\n",
    "\n",
    "def run_experiment_1_2_and_3(nn_base_arch,epochs,dense = False):\n",
    "    result = []\n",
    "    evaluations = []\n",
    "    for i in range(10):\n",
    "        checkpoint =\"../TFG/Modelos/balanced_model_\"+nn_base_arch+\"_exp1_v_\"+str(i)+\"_EXP0.h5\"\n",
    "        h,e = run_train(nn_base_arch,epochs,checkpoint,dense = False)\n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de 10\")\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,evaluations\n",
    "\n",
    "\n",
    "def run_experiment_4(nn_base_arch,epochs,dense = False):\n",
    "    result = []\n",
    "    result_post_tf = []\n",
    "    evaluations = []\n",
    "    evaluations_post_tf = []\n",
    "    for i in range(10):\n",
    "        checkpoint =\"../TFG/Modelos/balanced_model_\"+nn_base_arch+\"_exp4_v_\"+str(i)+\"_EXP0.h5\"\n",
    "        h,e,tf_model = run_train_w_model(nn_base_arch,epochs,checkpoint,dense = False)\n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "        \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "        layers = tf_model.layers[0:-1]\n",
    "        for layer in layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        h_retrain,e_retrain = re_train(tf_model,epochs)\n",
    "        \n",
    "        result_post_tf.append(h_retrain)\n",
    "        evaluations_post_tf.append(e_retrain)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de 10\")\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,result_post_tf,evaluations,evaluations_post_tf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos para hacer las gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_ex1_w_test_score(res,evaluations,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "            \n",
    "        plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\")\n",
    "        plt.plot(5,evaluations[i][1],'bo')\n",
    "        \n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_ex1(res,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "            \n",
    "        plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_experiments(res,res1,evaluations,evaluations1,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "        \n",
    "        precisiones1=[]\n",
    "        precisiones1.append(0.0)\n",
    "        for e in res1[i].history[\"accuracy\"]:\n",
    "            precisiones1.append(e)\n",
    "        \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\",color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"train_acc\",color='blue')\n",
    "        else:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "            plt.plot(5,evaluations[i][1],'bo',color='green')\n",
    "            plt.plot(5,evaluations1[i][1],'bo',color='blue')\n",
    "        \n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_all_experiments(res,res1,res2,res3,evaluations,evaluations1,evaluations2,evaluations3,epochs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        \n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "        \n",
    "        precisiones1=[]\n",
    "        precisiones1.append(0.0)\n",
    "        for e in res1[i].history[\"accuracy\"]:\n",
    "            precisiones1.append(e)\n",
    "            \n",
    "        precisiones2=[]\n",
    "        precisiones2.append(0.0)\n",
    "        for e in res2[i].history[\"accuracy\"]:\n",
    "            precisiones2.append(e)\n",
    "        \n",
    "        precisiones3=[]\n",
    "        precisiones3.append(0.0)\n",
    "        for e in res3[i].history[\"accuracy\"]:\n",
    "            precisiones3.append(e)\n",
    "            \n",
    "        \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"exp_1\",color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"exp_2\",color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1], label=\"exp_3\",color='brown')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1], label=\"exp_4\",color='salmon')\n",
    "        else:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1],color='brown')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1],color='salmon')\n",
    "\n",
    "        \n",
    "    plt.title(\"Training Accuracy and Test Results\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Exp_0_Results.jpg\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res1,res2,res3,res4,evaluations1,evaluations2,evaluations3,evaluations4\n",
    "\n",
    "precision1 = []\n",
    "precision2 = []\n",
    "precision3 = []\n",
    "precision4 = []\n",
    "\n",
    "for e in evaluations1:\n",
    "    element = e[1]\n",
    "    precision1.append(element)\n",
    "\n",
    "for e in evaluations2:\n",
    "    element = e[1]\n",
    "    precision2.append(element)\n",
    "\n",
    "for e in evaluations3:\n",
    "    element = e[1]\n",
    "    precision3.append(element)\n",
    "\n",
    "for e in evaluations5:\n",
    "    element = e[1]\n",
    "    precision4.append(element)\n",
    "\n",
    "    \n",
    "    \n",
    "d = {'Iteración':[1,2,3,4,5,6,7,8,9,10],'Precision 1': precision1,'Precision 2': precision2,'Precision 3': precision3,'Precision 4': precision4}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "print(df.mean())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vgg16vs19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
