{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsBFD7icF6Xc"
   },
   "source": [
    "# Experimento I : Validación del artículo \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\".\n",
    "\n",
    "En este experimiento usaremos la infraestructura planteada para comprobar si las ideas del artículo \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\" se validan en un problema multiclase. Usaremos los hiper-parámetros recomendados por el estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RxC7JJwlFuvk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkqu4FPPGQgV"
   },
   "source": [
    "## Parámetros globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZazLFjjGZVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoniojesus/Escritorio/TFG/Datos\n",
      "/home/antoniojesus/Escritorio/TFG/Datos/HAM10000_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "#Rutas de los datos.\n",
    " \n",
    "data_dir = os.path.dirname(os.path.realpath(\"../TFG/Datos/HAM10000_metadata.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.realpath(data_dir + \"/HAM10000_metadata.csv\")\n",
    "\n",
    "#Variables globales\n",
    "\n",
    "altura = 50\n",
    "longitud = 50\n",
    "clases = 7\n",
    "\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "print(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD9w48fEGfkF"
   },
   "source": [
    "## Creación del marco de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pf0IBVhhGehu",
    "outputId": "af59739a-413e-4a7d-c5ac-b775ab675e5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antoniojesus/Escritorio/TFG/Datos/HAM100...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antoniojesus/Escritorio/TFG/Datos/HAM100...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antoniojesus/Escritorio/TFG/Datos/HAM100...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antoniojesus/Escritorio/TFG/Datos/HAM100...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>/home/antoniojesus/Escritorio/TFG/Datos/HAM100...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "                                                path          cell_type  \\\n",
       "0  /home/antoniojesus/Escritorio/TFG/Datos/HAM100...  Benign keratosis    \n",
       "1  /home/antoniojesus/Escritorio/TFG/Datos/HAM100...  Benign keratosis    \n",
       "2  /home/antoniojesus/Escritorio/TFG/Datos/HAM100...  Benign keratosis    \n",
       "3  /home/antoniojesus/Escritorio/TFG/Datos/HAM100...  Benign keratosis    \n",
       "4  /home/antoniojesus/Escritorio/TFG/Datos/HAM100...  Benign keratosis    \n",
       "\n",
       "   cell_type_idx  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inicializando el dataFrame\n",
    "\n",
    "dataFrame=pd.read_csv(csv_path)\n",
    "\n",
    "#Mezclando carpetas.\n",
    "\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "\n",
    "# Inicializando diccionario de categorías\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "#Añadiendo columnas al dataFrame para que sea más legible.\n",
    "\n",
    "dataFrame['path'] = dataFrame['image_id'].map(imageid_path_dict.get)\n",
    "dataFrame['cell_type'] = dataFrame['dx'].map(lesion_type_dict.get) \n",
    "dataFrame['cell_type_idx'] = pd.Categorical(dataFrame['cell_type']).codes\n",
    "dataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1Py3CnE0bT",
    "outputId": "aea7fc1b-b1d9-42af-caa5-40293bbc2136"
   },
   "source": [
    "## Select_network\n",
    "\n",
    "Este método permite seleccionar la parte superior de una red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H9ySeIuMHak_"
   },
   "outputs": [],
   "source": [
    "def select_network(nn_base_arch):\n",
    "\n",
    "    if nn_base_arch =='CNN_SOCO':\n",
    "        nn = cnn_soco()\n",
    "        \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los métodos que se describen acontinuación permitirán crear diferentes tipos de capas superiores que se usarán en el método select_network. Los métodos son : \n",
    "\n",
    " - cnn_soco = Es una réplica de la red usada en el estudio \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_soco():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1), activation='relu',input_shape=(altura,longitud,3)))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1),activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    " \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos que se describen a continuación generarán la capa de salida de nuestra red : \n",
    " - build : Modificación de la anterior, añadimos una capa de BachNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(nn):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(nn)\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(128))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(clases,activation='softmax'))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializar modelo\n",
    "\n",
    "nn = select_network('CNN_SOCO')\n",
    "model = build(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se procede a crear un método que permita balancear la carga de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NO6Snfw-KYe4"
   },
   "outputs": [],
   "source": [
    "def balanced_dataset(df):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    #df = pd.DataFrame()\n",
    "    \n",
    "    for cat in df['cell_type_idx'].unique():\n",
    "        temp = resample(df[df['cell_type_idx'] == cat], \n",
    "                        replace=True,     # sample with replacement\n",
    "                        n_samples=2500,   # to match majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df_balanced = pd.concat([df_balanced, temp])\n",
    " \n",
    "    df_balanced['cell_type'].value_counts()\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def load_img_data(size, df, balanced=False):\n",
    "    \"\"\"\n",
    "        ..\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "    \n",
    "    img_h, img_w = size, size\n",
    "    imgs = []\n",
    "    \n",
    "    if balanced:\n",
    "        df = balanced_dataset(df)\n",
    "    \n",
    "    image_paths = list(df['path'])\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=0)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    #imgs = imgs.astype(np.float32) / 255.\n",
    "    \n",
    "    return imgs, df['cell_type_idx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LMBlVsJN-5o"
   },
   "outputs": [],
   "source": [
    "del dataFrame\n",
    "del imgs\n",
    "del target\n",
    "del x_train\n",
    "del x_test\n",
    "del y_train\n",
    "del y_test\n",
    "del x_val\n",
    "del y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos y generamos información necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_general_data():\n",
    "    '''Carga los datos de forma general para proyecto'''\n",
    "    \n",
    "    imgs, target = load_img_data(altura, dataFrame, balanced=True)\n",
    "    \n",
    "    x_train, x_transferLearning, y_train, y_transferLearning = train_test_split(imgs, target, test_size=0.60)\n",
    "       \n",
    "    source_data = [ x_transferLearning , y_transferLearning ]\n",
    "    target_data = [ x_train , y_train ]\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(target_data[0], target_data[1], test_size=0.70)\n",
    "    \n",
    "    train_data = [x_train,y_train]\n",
    "    test_data = [x_test,y_test]\n",
    "    \n",
    "    return source_data,train_data,test_data\n",
    "\n",
    "\n",
    "def get_data_for_ex_1(source_data,train_data,test_data):\n",
    "    '''Carga los específicos del proyecto para el experimento 1'''\n",
    "    \n",
    "    x_train = train_data[0]\n",
    "    y_train = train_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_2(source_data,train_data,test_data):\n",
    "    '''Carga los específicos del proyecto para el experimento 2'''\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_3(source_data,train_data,test_data):\n",
    "    '''Carga los específicos del proyecto para el experimento 3'''\n",
    "    \n",
    "    data_0 = source_data[0]\n",
    "    data_1 = source_data[1]\n",
    "    \n",
    "    for e in train_data[0]:\n",
    "        np.append(data_0,e)\n",
    "        \n",
    "    for e in train_data[1]:\n",
    "        np.append(data_1,e)\n",
    "        \n",
    "    x_train = data_0\n",
    "    y_train = data_1\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_data_for_ex_4(source_data,train_data,test_data):\n",
    "    ''''Carga los específicos del proyecto para el experimento 4'''\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_retrain = train_data[0]\n",
    "    y_retrain = train_data[1]\n",
    "    \n",
    "    x_test = test_data[0]\n",
    "    y_test = test_data[1]\n",
    "    \n",
    "    return x_train,x_retrain,x_test,y_train,y_retrain,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constantes\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "RMSpropEstudio = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17500/17500 [04:38<00:00, 62.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "#Cargamos datos generales\n",
    "source_data,train_data,test_data = load_general_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corremos todos los experimentos\n",
    "\n",
    "# Los primeros tres experimento se ejecutan con run_experiment_1_2_and_3 pues no necesitan realizar un proceso de transfer learning.\n",
    "# El experimento 4 usa run_experiment_4 puesto que necesita transfer learning.\n",
    "\n",
    "x_train,x_test,y_train,y_test = get_data_for_ex_1(source_data,train_data,test_data)\n",
    "res1,evaluations1 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "x_train,x_test,y_train,y_test = get_data_for_ex_2(source_data,train_data,test_data)\n",
    "res2,evaluations2 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "x_train,x_test,y_train,y_test = get_data_for_ex_3(source_data,train_data,test_data)\n",
    "res3,evaluations3 = run_experiment_1_2_and_3(nn_base_arch,EPOCHS)\n",
    "\n",
    "x_train,x_retrain,x_test,y_train,y_retrain,y_test = get_data_for_ex_4(source_data,train_data,test_data)\n",
    "res4,res5,evaluations4,evaluations5 = run_experiment_4(nn_base_arch,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSpropEstudio, metrics=['accuracy','mse'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[cpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation\n",
    "\n",
    "def run_train_w_model(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSpropEstudio, metrics=['accuracy','mse'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[cpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation,model\n",
    "\n",
    "def re_train(model,epocas):\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,batch_size = BATCH_SIZE,verbose=0)\n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "    return history,evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_1_2_and_3(nn_base_arch,epochs,dense = False):\n",
    "    result = []\n",
    "    evaluations = []\n",
    "    for i in range(10):\n",
    "        checkpoint =\"../TFG/Modelos/balanced_model_\"+nn_base_arch+\"_exp1_v_\"+str(i)+\"_EXP0.h5\"\n",
    "        h,e = run_train(nn_base_arch,epochs,checkpoint,dense = False)\n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de 10\")\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,evaluations\n",
    "\n",
    "\n",
    "def run_experiment_4(nn_base_arch,epochs,dense = False):\n",
    "    result = []\n",
    "    result_post_tf = []\n",
    "    evaluations = []\n",
    "    evaluations_post_tf = []\n",
    "    for i in range(10):\n",
    "        checkpoint =\"../TFG/Modelos/balanced_model_\"+nn_base_arch+\"_exp4_v_\"+str(i)+\"_EXP0.h5\"\n",
    "        h,e,tf_model = run_train_w_model(nn_base_arch,epochs,checkpoint,dense = False)\n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "\n",
    "        layers = tf_model.layers[0:-1]\n",
    "        for layer in layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        h_retrain,e_retrain = re_train(tf_model,epochs)\n",
    "        \n",
    "        result_post_tf.append(h_retrain)\n",
    "        evaluations_post_tf.append(e_retrain)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de 10\")\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,result_post_tf,evaluations,evaluations_post_tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_all_experiments(res,res1,res2,res3,evaluations,evaluations1,evaluations2,evaluations3,epochs):\n",
    "    '''Representamos una gráfica en la que se comparan los resultados del entrenamiento '''\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        \n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "        \n",
    "        precisiones1=[]\n",
    "        precisiones1.append(0.0)\n",
    "        for e in res1[i].history[\"accuracy\"]:\n",
    "            precisiones1.append(e)\n",
    "            \n",
    "        precisiones2=[]\n",
    "        precisiones2.append(0.0)\n",
    "        for e in res2[i].history[\"accuracy\"]:\n",
    "            precisiones2.append(e)\n",
    "        \n",
    "        precisiones3=[]\n",
    "        precisiones3.append(0.0)\n",
    "        for e in res3[i].history[\"accuracy\"]:\n",
    "            precisiones3.append(e)\n",
    "            \n",
    "        \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"exp_1\",color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"exp_2\",color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1], label=\"exp_3\",color='brown')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1], label=\"exp_4\",color='salmon')\n",
    "        else:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1],color='brown')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1],color='salmon')\n",
    "\n",
    "        \n",
    "    plt.title(\"Training Accuracy and Test Results\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Exp_0_Results.jpg\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_all_experiments(res1,res2,res3,res5,evaluations1,evaluations2,evaluations3,evaluations5,EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos un marco de datos con panda para organizar los resultados y realizar calculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración      5.500000\n",
      "Precision 1    0.333837\n",
      "Precision 2    0.385735\n",
      "Precision 3    0.351204\n",
      "Precision 4    0.435306\n",
      "dtype: float64\n",
      "0    0.432449\n",
      "1    0.446531\n",
      "2    0.473878\n",
      "3    0.337959\n",
      "4    0.428163\n",
      "5    0.443673\n",
      "6    0.503878\n",
      "7    0.350612\n",
      "8    0.454898\n",
      "9    0.481020\n",
      "Name: Precision 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#res1,res2,res3,res4,evaluations1,evaluations2,evaluations3,evaluations4\n",
    "\n",
    "precision1 = []\n",
    "precision2 = []\n",
    "precision3 = []\n",
    "precision4 = []\n",
    "\n",
    "for e in evaluations1:\n",
    "    element = e[1]\n",
    "    precision1.append(element)\n",
    "\n",
    "for e in evaluations2:\n",
    "    element = e[1]\n",
    "    precision2.append(element)\n",
    "\n",
    "for e in evaluations3:\n",
    "    element = e[1]\n",
    "    precision3.append(element)\n",
    "\n",
    "for e in evaluations5:\n",
    "    element = e[1]\n",
    "    precision4.append(element)\n",
    "\n",
    "    \n",
    "    \n",
    "d = {'Iteración':[1,2,3,4,5,6,7,8,9,10],'Precision 1': precision1,'Precision 2': precision2,'Precision 3': precision3,'Precision 4': precision4}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "print(df.mean(0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vgg16vs19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
