# -*- coding: utf-8 -*-
"""TFG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yuG6N5y0oL5Dcmj4OloEEmtFLWImN5_u
"""

import numpy as np
import pandas as pd
import seaborn as sns
from glob import glob
from matplotlib import pyplot as plt
import os
import tensorflow as tf
from tqdm import tqdm
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils import resample

#Conexión con google drive

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

#Rutas de los datos.
 
data_path =  '/content/drive/My Drive/Universidad/TFG/Datos/'
data_dir = '../content/drive/My Drive/Universidad/TFG/Datos/'

csv_path = data_path + 'HAM10000_metadata.csv'

#Variables globales

altura = 50
longitud = 50
clases = 7
batch_size = 100

#Creando el dataFrame

dataFrame=pd.read_csv(csv_path)
dataFrame.head()

#Mezclando carpetas.
all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))
imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}

# Diccionario de categorías

lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

#Añadiendo columnas al dataFrame para que sea más legible.
dataFrame['path'] = dataFrame['image_id'].map(imageid_path_dict.get)
dataFrame['cell_type'] = dataFrame['dx'].map(lesion_type_dict.get) 
dataFrame['cell_type_idx'] = pd.Categorical(dataFrame['cell_type']).codes
dataFrame.head()

cases_count = dataFrame.cell_type.value_counts()
print(cases_count)

#Mostrando incidencia

plt.figure(figsize=(14,10))
sns.barplot(x=cases_count.index, y= cases_count.values)
plt.title('Number of cases', fontsize=14)
plt.xlabel('Case type', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(range(len(cases_count.index)),lesion_type_dict.get(0) )
plt.show()

def getClass(path):
  seg = path.split("/")
  seg[-1]
  seg = seg[-1].split(".")

  res = df.iloc[df.index.get_loc(seg[0]), 1]
  return res

from glob import glob
f, ax = plt.subplots(16,5, figsize=(30,40))
for i in range(80):
    
    if i<5:
        img1 = glob(data_path + "HAM10000_images_part_1/*.jpg")  
        path = img1[i]          
        img2 = np.asarray(plt.imread(path))

        ax[i//5, i%5].imshow(img2, cmap='gray') 
        ax[i//5, i%5].set_title(getClass(path))

    else:
        img1 = glob(data_path + "HAM10000_images_part_2/*.jpg")  
        path = img1[i]          
        img2 = np.asarray(plt.imread(path))

        ax[i//5, i%5].imshow(img2, cmap='gray') 
        ax[i//5, i%5].set_title(getClass(path))

    ax[i//5, i%5].axis('off')
    ax[i//5, i%5].set_aspect('auto')
plt.show()

"""Comenzamos instalando efficientnet"""

pip install -U efficientnet

#Import de efficientnet

import efficientnet.keras as efn

#Cargando el modelo pre-entrenado
nn = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(altura, longitud,3))
pre_trained_model = efn.EfficientNetB4(
    include_top=False, weights='imagenet', input_tensor=None,input_shape=(altura, longitud,3), classes=clases
)
for layer in pre_trained_model.layers:
  layer.trainable = False

modelENB7 = tf.keras.Sequential()
modelENB7.add(pre_trained_model)
modelENB7.add(tf.keras.layers.Flatten())
modelENB7.add(tf.keras.layers.Dense(500,activation='relu'))
modelENB7.add(tf.keras.layers.Dense(500,activation='relu'))
modelENB7.add(tf.keras.layers.Dropout(0.2))
modelENB7.add(tf.keras.layers.Dense(7,activation='softmax'))

print(modelENB7.summary())

import tensorflow as tf
Adam = tf.keras.optimizers.Adam(
    learning_rate=0.01,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name="Adam"
)

def balanced_dataset(df):
    df_balanced = pd.DataFrame()
    #df = pd.DataFrame()
    
    for cat in df['cell_type_idx'].unique():
        temp = resample(df[df['cell_type_idx'] == cat], 
                        replace=True,     # sample with replacement
                        n_samples=7000,   # to match majority class
                        random_state=123) # reproducible results

        # Combine majority class with upsampled minority class
        df_balanced = pd.concat([df_balanced, temp])
 
    df_balanced['cell_type'].value_counts()

    return df_balanced

def load_img_data(size, df, balanced=False):
    """
        ..
        first we should normalize the image from 0-255 to 0-1
    """
    
    img_h, img_w = size, size
    imgs = []
    
    if balanced:
        df = balanced_dataset(df)
    
    image_paths = list(df['path'])

    for i in tqdm(range(len(image_paths))):
        img = cv2.imread(image_paths[i])
        img = cv2.resize(img, (img_h, img_w))
        img = img.astype(np.float32) / 255.
        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))
        imgs.append(img)

    imgs = np.stack(imgs, axis=0)
    print(imgs.shape)

    #imgs = imgs.astype(np.float32) / 255.
    
    return imgs, df['cell_type_idx'].values



imgs, target = load_img_data(50, dataFrame, True)



x_train, x_test, y_train, y_test = train_test_split(imgs, target, test_size=0.20)
x_train, x_val, y_train, y_val = train_test_split(imgs, target, test_size=0.05)

train_val_test = (x_train, y_train, x_val, y_val, x_test, y_test)

[x_train.shape, x_val.shape, x_test.shape]

"""Definimos la función de callback ModelCheckpoint"""

best_weights_ph1 ="balanced_modelENB4_ph1_weights.hdf5"

checkpoint = tf.keras.callbacks.ModelCheckpoint(best_weights_ph1, monitor="val_loss", mode="min", save_best_only=True, verbose=1)

modelENB7.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy','mse'])

pasos_validacion = 12
epocas = 10
pasos = 128

trainX = train_val_test[0]
trainY = train_val_test[1]
valX = train_val_test[2]
valY = train_val_test[3]
testX = train_val_test[4]
testY = train_val_test[5]

numTrainingSamples = trainX.shape[0]
  numValidationSamples = valX.shape[0]

modelENB7.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])

history = modelENB7.fit(
            x_train, y_train,
            steps_per_epoch=numTrainingSamples ,
            epochs=10,
            validation_data=(valX, valY),
            validation_steps=numValidationSamples ,
            callbacks=[checkpoint])

