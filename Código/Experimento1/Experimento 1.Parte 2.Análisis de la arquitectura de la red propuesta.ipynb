{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsBFD7icF6Xc"
   },
   "source": [
    "# Experimento 1.Parte 2.Análisis de la arquitectura usada en \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\"\n",
    "\n",
    "En este experimento comprobaremos como afecta la implementación de capas de dropout, batchnormalization y funciones de activación ReLU sobre la red propuesta en \"A Preliminary Study on Deep Transfer Learning Applied to Image Classification for Small Datasets\" con los hiperparámetros elegidos en el Experimento 1.Parte 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RxC7JJwlFuvk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math \n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkqu4FPPGQgV"
   },
   "source": [
    "## Definición de rutas y variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CZazLFjjGZVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antgarnie/Escritorio/TFG/Datos\n",
      "/home/antgarnie/Escritorio/TFG/Datos/HAM10000_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "#Rutas de los datos.\n",
    " \n",
    "data_dir = os.path.dirname(os.path.realpath(\"../TFG/Datos/HAM10000_metadata.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.realpath(data_dir + \"/HAM10000_metadata.csv\")\n",
    "\n",
    "#Variables globales\n",
    "\n",
    "altura = 50\n",
    "longitud = 50\n",
    "clases = 7\n",
    "\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "print(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD9w48fEGfkF"
   },
   "source": [
    "## Creación del marco de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pf0IBVhhGehu",
    "outputId": "af59739a-413e-4a7d-c5ac-b775ab675e5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "                                                path          cell_type  \\\n",
       "0  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "1  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "2  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "3  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "4  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "\n",
       "   cell_type_idx  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inicializando el dataFrame\n",
    "\n",
    "dataFrame=pd.read_csv(csv_path)\n",
    "\n",
    "#Mezclando carpetas.\n",
    "\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "\n",
    "# Inicializando diccionario de categorías\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "#Añadiendo columnas al dataFrame para que sea más legible.\n",
    "\n",
    "dataFrame['path'] = dataFrame['image_id'].map(imageid_path_dict.get)\n",
    "dataFrame['cell_type'] = dataFrame['dx'].map(lesion_type_dict.get) \n",
    "dataFrame['cell_type_idx'] = pd.Categorical(dataFrame['cell_type']).codes\n",
    "dataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1Py3CnE0bT",
    "outputId": "aea7fc1b-b1d9-42af-caa5-40293bbc2136"
   },
   "source": [
    "## Preparación de la red\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_build_cnn_soco(withBatchNormalization = False, withDropout=False):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1),input_shape=(altura,longitud,3)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    " \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(128))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withDropout):\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(clases,activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se procede a crear un método que permita balancear la carga de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NO6Snfw-KYe4"
   },
   "outputs": [],
   "source": [
    "def balanced_dataset(df):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    #df = pd.DataFrame()\n",
    "    \n",
    "    for cat in df['cell_type_idx'].unique():\n",
    "        temp = resample(df[df['cell_type_idx'] == cat], \n",
    "                        replace=True,     # sample with replacement\n",
    "                        n_samples=2500,   # to match majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df_balanced = pd.concat([df_balanced, temp])\n",
    " \n",
    "    df_balanced['cell_type'].value_counts()\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def load_img_data(size, df, balanced=False):\n",
    "    \"\"\"\n",
    "        ..\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "    \n",
    "    img_h, img_w = size, size\n",
    "    imgs = []\n",
    "    \n",
    "    if balanced:\n",
    "        df = balanced_dataset(df)\n",
    "    \n",
    "    image_paths = list(df['path'])\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=0)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    #imgs = imgs.astype(np.float32) / 255.\n",
    "    \n",
    "    return imgs, df['cell_type_idx'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos y creamos los casos a experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_general_data():\n",
    "    \n",
    "    imgs, target = load_img_data(altura, dataFrame, balanced=True)\n",
    "    \n",
    "    x_train, x_transferLearning, y_train, y_transferLearning = train_test_split(imgs, target, test_size=0.60)\n",
    "       \n",
    "    source_data = [ x_transferLearning , y_transferLearning ]\n",
    "    target_data = [ x_train , y_train ]\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(target_data[0], target_data[1], test_size=0.70)\n",
    "    \n",
    "    train_data = [x_train,y_train]\n",
    "    test_data = [x_test,y_test]\n",
    "    \n",
    "    return source_data,train_data,test_data\n",
    "\n",
    "\n",
    "def get_data_for_ex(source_data,train_data,test_data):\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_retrain = train_data[0]\n",
    "    y_retrain = train_data[1]\n",
    "    \n",
    "    percent = math.floor(len(test_data[0])/100*30)\n",
    "       \n",
    "    x_validation = test_data[0][0:percent]\n",
    "    y_validation = test_data[1][0:percent]\n",
    "    \n",
    "    \n",
    "    x_test = test_data[0][percent:-1]\n",
    "    y_test = test_data[1][percent:-1]\n",
    "    \n",
    "    return x_train,x_retrain,x_test,x_validation,y_train,y_retrain,y_test,y_validation\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Definimos los parámetros globales #\n",
    "#####################################\n",
    "\n",
    "ITERATIONS_PER_EXP = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17500/17500 [01:58<00:00, 148.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "source_data,train_data,test_data = load_general_data()\n",
    "x_train,x_retrain,x_test,x_validation,y_train,y_retrain,y_test,y_validation = get_data_for_ex(source_data,train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimento 1. Red soco con Dropout.\n",
    "res11,res12,evaluations,evaluations1 = run_experiment(0,20)\n",
    "\n",
    "#Experimento 2. Red soco con BN y Dropout.\n",
    "res21,res22,evaluations,evaluations2 = run_experiment(1,15)\n",
    "\n",
    "#Experimento 3. Red soco con BN y sin Dropout.\n",
    "res31,res32,evaluations,evaluations3 = run_experiment(2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_w_model(model,epochs,output_layer):\n",
    "        \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001,amsgrad=True)\n",
    "        \n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                patience = 3,\n",
    "                                                mode = 'min')    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,validation_data=(x_validation, y_validation),epochs=epochs,\n",
    "                        callbacks=[earlyStopping],batch_size = BATCH_SIZE)\n",
    "    \n",
    "    print(len(x_test))\n",
    "    \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation,model\n",
    "    \n",
    "def re_train(model,checkpoint,epochs):\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001,amsgrad=True)\n",
    "\n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                patience = 3,\n",
    "                                                mode = 'min')    \n",
    "    \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                mode=\"min\",\n",
    "                                                save_best_only=True,\n",
    "                                                verbose=0)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,validation_data=(x_validation, y_validation),epochs=epochs,callbacks=[earlyStopping,cpoint],batch_size = BATCH_SIZE)\n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "    return history,evaluation,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(index_model,epoch,iterations = ITERATIONS_PER_EXP):\n",
    "    result = []\n",
    "    result_post_tf = []\n",
    "    evaluations = []\n",
    "    evaluations_post_tf = []\n",
    "    dense = False\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        checkpoint =\"../TFG/Modelos/balanced_model_functionalv2.h5\"\n",
    "        \n",
    "        if index_model == 0:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = False, withDropout=True)\n",
    "        if index_model == 1:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = True, withDropout=True)\n",
    "        if index_model == 2:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = True, withDropout=False)\n",
    "        \n",
    "        h,e,tf_model = run_train_w_model(model,epoch,dense)\n",
    "        \n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "\n",
    "        layers = tf_model.layers[0:-1]\n",
    "        for layer in layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        h_retrain,e_retrain,model = re_train(tf_model,checkpoint,epoch)\n",
    "\n",
    "        result_post_tf.append(h_retrain)\n",
    "        evaluations_post_tf.append(e_retrain)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de \"+ str(iterations))\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,result_post_tf,evaluations,evaluations_post_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de representación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_ex1_w_test_score(res,evaluations,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "            \n",
    "        plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\")\n",
    "        plt.plot(5,evaluations[i][1],'bo')\n",
    "        \n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_ex1(res,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "            \n",
    "        plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_for_experiments(res,res1,evaluations,evaluations1,epochs,name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(res)):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[\"accuracy\"]:\n",
    "            precisiones.append(e)\n",
    "        \n",
    "        precisiones1=[]\n",
    "        precisiones1.append(0.0)\n",
    "        for e in res1[i].history[\"accuracy\"]:\n",
    "            precisiones1.append(e)\n",
    "        \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_acc\",color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"train_acc\",color='blue')\n",
    "        else:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "            plt.plot(5,evaluations[i][1],'bo',color='green')\n",
    "            plt.plot(5,evaluations1[i][1],'bo',color='blue')\n",
    "        \n",
    "    plt.title(\"Training Loss and Accuracy - {}\".format(name))\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.savefig(\"Exp_1_\"+\"Training \"+str(value)+\" and validation \"+str(value)+\".jpg\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trainAcc_vs_valAcc_for_experiments(res,res1,epochs,name,value=\"loss\"):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    for i in range(len(range(5))):\n",
    "        precisiones=[]\n",
    "        precisiones.append(0.0)\n",
    "        for e in res[i].history[value]:\n",
    "            precisiones.append(e)\n",
    "            \n",
    "        #precisiones.append(0.0)\n",
    "        for e in res1[i].history[value]:\n",
    "            precisiones.append(e)\n",
    "        \n",
    "        precisiones1=[]\n",
    "        precisiones1.append(0.0)\n",
    "        for e in res[i].history[\"val_\"+str(value)]:\n",
    "            precisiones1.append(e)\n",
    "\n",
    "        #precisiones1.append(0.0)\n",
    "        for e in res1[i].history[\"val_\"+str(value)]:\n",
    "            precisiones1.append(e)\n",
    "        \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"train_\"+str(value),color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"val_\"+str(value),color='blue')\n",
    "        else:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "        \n",
    "    plt.title(\"Training \"+str(value)+\" and validation \"+str(value))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(str(value).capitalize())\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Exp_1_BN_\"+\"Training \"+str(value)+\" and validation \"+str(value)+\".jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trainAcc_vs_valAcc_for_experiments(res31,res32,17,\"\",\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representamos gráficamente los resultados obtenidos al experimentar con el tamaño del lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values(resx1,resx2,i,val):\n",
    "    res=[]\n",
    "    if val != \"val_loss\":\n",
    "        res.append(0.0)\n",
    "    for e in resx1[i].history[val]:\n",
    "        res.append(e)\n",
    "    for e in resx2[i].history[val]:\n",
    "        res.append(e)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_acc_all_experiments(res11,res12,res21,res22,res31,res32,epochs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    acc = \"val_accuracy\"\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        precisiones = compute_values(res11,res12,i,acc)\n",
    "            \n",
    "        precisiones1 = compute_values(res21,res22,i,acc)\n",
    "        \n",
    "        precisiones2 = compute_values(res31,res32,i,acc)\n",
    "        \n",
    "        \n",
    "        loss = compute_values(res11,res12,i,\"val_loss\")\n",
    "        \n",
    "        loss1 = compute_values(res21,res22,i,\"val_loss\")\n",
    "        \n",
    "        loss2 = compute_values(res31,res32,i,\"val_loss\")\n",
    "        \n",
    "\n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"Dropout\",color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"BachNormalization y Dropout\",color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1], label=\"BachNormalization sin Dropout\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1], label=\"Amsgrad\",color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), loss[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss1[0:epochs+1],color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), loss2[0:epochs+1],color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1],color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1],color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), loss[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss1[0:epochs+1],color='blue')\n",
    "            plt.plot(np.arange(0, epochs+1), loss2[0:epochs+1],color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss4[0:epochs+1],color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss5[0:epochs+1],color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss6[0:epochs+1],color='yellowgreen')\n",
    "            \n",
    "        del precisiones\n",
    "        del precisiones1\n",
    "        del precisiones2\n",
    "\n",
    "\n",
    "        \n",
    "    plt.title(\"Training Accuracy and Test Results\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Exp_1_2_kinds.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_all_experiments(res11,res12,res21,res22,res31,res32,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos marcos de datos para analizar los resultados de evaluar los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision1 = []\n",
    "precision2 = []\n",
    "precision3 = []\n",
    "precision4 = []\n",
    "precision5 = []\n",
    "\n",
    "\n",
    "for e in evaluations1:\n",
    "    element = e[1]\n",
    "    precision1.append(element)\n",
    "    \n",
    "for e in evaluations2:\n",
    "    element = e[1]\n",
    "    precision2.append(element)\n",
    "    \n",
    "for e in evaluations3:\n",
    "    element = e[1]\n",
    "    precision3.append(element)\n",
    "\n",
    "    \n",
    "d = {'Dropout': precision1,'BachNormalization y Dropout': precision2,'BachNormalization sin Dropout':precision3}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "print(df)\n",
    "print(df.mean())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vgg16vs19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
