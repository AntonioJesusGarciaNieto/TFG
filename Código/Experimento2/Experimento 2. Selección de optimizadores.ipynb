{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsBFD7icF6Xc"
   },
   "source": [
    "# Experimento 2 : Análisis de optimizadores para la red soco optimizada.\n",
    "\n",
    "En este experimento comprobaremos que optimizador tienen un mejor desempeño teniendo en cuenta nuestro conjunto de datos y las mejoras implementadas a la red SOCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RxC7JJwlFuvk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math \n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkqu4FPPGQgV"
   },
   "source": [
    "## Definición de rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CZazLFjjGZVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antgarnie/Escritorio/TFG/Datos\n",
      "/home/antgarnie/Escritorio/TFG/Datos/HAM10000_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "#Rutas de los datos.\n",
    " \n",
    "data_dir = os.path.dirname(os.path.realpath(\"../TFG/Datos/HAM10000_metadata.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.realpath(data_dir + \"/HAM10000_metadata.csv\")\n",
    "\n",
    "#Variables globales\n",
    "\n",
    "altura = 50\n",
    "longitud = 50\n",
    "clases = 7\n",
    "\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "print(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD9w48fEGfkF"
   },
   "source": [
    "## Creación del marco de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pf0IBVhhGehu",
    "outputId": "af59739a-413e-4a7d-c5ac-b775ab675e5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>/home/antgarnie/Escritorio/TFG/Datos/HAM10000_...</td>\n",
       "      <td>Benign keratosis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "                                                path          cell_type  \\\n",
       "0  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "1  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "2  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "3  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "4  /home/antgarnie/Escritorio/TFG/Datos/HAM10000_...  Benign keratosis    \n",
       "\n",
       "   cell_type_idx  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inicializando el dataFrame\n",
    "\n",
    "dataFrame=pd.read_csv(csv_path)\n",
    "\n",
    "#Mezclando carpetas.\n",
    "\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "\n",
    "# Inicializando diccionario de categorías\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "#Añadiendo columnas al dataFrame para que sea más legible.\n",
    "\n",
    "dataFrame['path'] = dataFrame['image_id'].map(imageid_path_dict.get)\n",
    "dataFrame['cell_type'] = dataFrame['dx'].map(lesion_type_dict.get) \n",
    "dataFrame['cell_type_idx'] = pd.Categorical(dataFrame['cell_type']).codes\n",
    "dataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1Py3CnE0bT",
    "outputId": "aea7fc1b-b1d9-42af-caa5-40293bbc2136"
   },
   "source": [
    "## Preparación de la red\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_build_cnn_soco(withBatchNormalization = False, withDropout=False):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1),input_shape=(altura,longitud,3)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3),(1,1)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    " \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3)))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withBatchNormalization):\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(128))\n",
    "    model.add(tf.keras.layers.PReLU())\n",
    "    if(withDropout):\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(clases,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se procede a crear un método que permita balancear la carga de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NO6Snfw-KYe4"
   },
   "outputs": [],
   "source": [
    "def balanced_dataset(df):\n",
    "    df_balanced = pd.DataFrame()\n",
    "    #df = pd.DataFrame()\n",
    "    \n",
    "    for cat in df['cell_type_idx'].unique():\n",
    "        temp = resample(df[df['cell_type_idx'] == cat], \n",
    "                        replace=True,     # sample with replacement\n",
    "                        n_samples=2500,   # to match majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df_balanced = pd.concat([df_balanced, temp])\n",
    " \n",
    "    df_balanced['cell_type'].value_counts()\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def load_img_data(size, df, balanced=False):\n",
    "    \"\"\"\n",
    "        ..\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "    \n",
    "    img_h, img_w = size, size\n",
    "    imgs = []\n",
    "    \n",
    "    if balanced:\n",
    "        df = balanced_dataset(df)\n",
    "    \n",
    "    image_paths = list(df['path'])\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=0)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    #imgs = imgs.astype(np.float32) / 255.\n",
    "    \n",
    "    return imgs, df['cell_type_idx'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos y creamos los casos a experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_general_data():\n",
    "    \n",
    "    imgs, target = load_img_data(altura, dataFrame, balanced=True)\n",
    "    \n",
    "    x_train, x_transferLearning, y_train, y_transferLearning = train_test_split(imgs, target, test_size=0.60)\n",
    "       \n",
    "    source_data = [ x_transferLearning , y_transferLearning ]\n",
    "    target_data = [ x_train , y_train ]\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(target_data[0], target_data[1], test_size=0.70)\n",
    "    \n",
    "    train_data = [x_train,y_train]\n",
    "    test_data = [x_test,y_test]\n",
    "    \n",
    "    return source_data,train_data,test_data\n",
    "\n",
    "\n",
    "def get_data_for_ex(source_data,train_data,test_data):\n",
    "    \n",
    "    x_train = source_data[0]\n",
    "    y_train = source_data[1]\n",
    "    \n",
    "    x_retrain = train_data[0]\n",
    "    y_retrain = train_data[1]\n",
    "    \n",
    "    percent = math.floor(len(test_data[0])/100*30)\n",
    "       \n",
    "    x_validation = test_data[0][0:percent]\n",
    "    y_validation = test_data[1][0:percent]\n",
    "    \n",
    "    \n",
    "    x_test = test_data[0][percent:-1]\n",
    "    y_test = test_data[1][percent:-1]\n",
    "    \n",
    "    return x_train,x_retrain,x_test,x_validation,y_train,y_retrain,y_test,y_validation\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# Definimos 7 experimentos cada uno con un optimizador distinto y definimos el número de iteraciones          #\n",
    "###############################################################################################################\n",
    "\n",
    "ITERATIONS_PER_EXP = 5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "def set_hiper_to_exp_1(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)  \n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "\n",
    "def set_hiper_to_exp_2(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "    \n",
    "def set_hiper_to_exp_3(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "\n",
    "def set_hiper_to_exp_4(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,amsgrad=True)\n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "    \n",
    "def set_hiper_to_exp_5(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=LEARNING_RATE)\n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "\n",
    "def set_hiper_to_exp_6(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "    return BATCH_SIZE,EPOCHS,opt\n",
    "\n",
    "def set_hiper_to_exp_7(BATCH_SIZE,EPOCHS,LEARNING_RATE):\n",
    "    opt = tf.keras.optimizers.Adadelta(learning_rate=LEARNING_RATE)  \n",
    "    return BATCH_SIZE,EPOCHS,opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17500/17500 [01:50<00:00, 157.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "source_data,train_data,test_data = load_general_data()\n",
    "x_train,x_retrain,x_test,x_validation,y_train,y_retrain,y_test,y_validation = get_data_for_ex(source_data,train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 48, 48, 32)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 46, 46, 32)        9248      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 46, 46, 32)        67712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 21, 21, 64)        28224     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,019,175\n",
      "Trainable params: 1,018,919\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "657/657 [==============================] - 5s 7ms/step - loss: 0.9594 - accuracy: 0.6693 - val_loss: 0.6308 - val_accuracy: 0.7857\n",
      "Epoch 2/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.3699 - accuracy: 0.8776 - val_loss: 0.4306 - val_accuracy: 0.8653\n",
      "Epoch 3/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.1865 - accuracy: 0.9386 - val_loss: 0.3307 - val_accuracy: 0.9000\n",
      "Epoch 4/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0924 - accuracy: 0.9713 - val_loss: 0.3309 - val_accuracy: 0.9000\n",
      "Epoch 5/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0516 - accuracy: 0.9847 - val_loss: 0.4127 - val_accuracy: 0.8925\n",
      "Epoch 6/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.3354 - val_accuracy: 0.9245\n",
      "Epoch 7/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.3540 - val_accuracy: 0.9204\n",
      "Epoch 8/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0135 - accuracy: 0.9964 - val_loss: 0.3839 - val_accuracy: 0.9259\n",
      "Epoch 9/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 0.3750 - val_accuracy: 0.9286\n",
      "Epoch 10/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.3818 - val_accuracy: 0.9320\n",
      "Epoch 11/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.5149 - val_accuracy: 0.9184\n",
      "Epoch 12/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.5790 - val_accuracy: 0.9197\n",
      "Epoch 13/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.6631 - val_accuracy: 0.9116\n",
      "Epoch 14/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.5967 - val_accuracy: 0.9190\n",
      "Epoch 15/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.5873 - val_accuracy: 0.9231\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4970 - accuracy: 0.9294\n",
      "--------------------------------------------------------------------------\n",
      "Epoch 1/15\n",
      "657/657 - 4s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.5734 - val_accuracy: 0.9265\n",
      "Epoch 2/15\n",
      "657/657 - 4s - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.5474 - val_accuracy: 0.9272\n",
      "Epoch 3/15\n",
      "657/657 - 4s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.5000 - val_accuracy: 0.9333\n",
      "Epoch 4/15\n",
      "657/657 - 5s - loss: 8.1337e-04 - accuracy: 0.9998 - val_loss: 0.6001 - val_accuracy: 0.9286\n",
      "Epoch 5/15\n",
      "657/657 - 6s - loss: 8.9462e-04 - accuracy: 0.9997 - val_loss: 0.5723 - val_accuracy: 0.9211\n",
      "Epoch 6/15\n",
      "657/657 - 5s - loss: 6.9548e-04 - accuracy: 0.9998 - val_loss: 0.6374 - val_accuracy: 0.9286\n",
      "Epoch 7/15\n",
      "657/657 - 5s - loss: 6.0829e-04 - accuracy: 0.9999 - val_loss: 0.6151 - val_accuracy: 0.9088\n",
      "Epoch 8/15\n",
      "657/657 - 5s - loss: 4.0818e-04 - accuracy: 0.9999 - val_loss: 0.6603 - val_accuracy: 0.9259\n",
      "Epoch 9/15\n",
      "657/657 - 5s - loss: 3.7609e-04 - accuracy: 0.9999 - val_loss: 0.6852 - val_accuracy: 0.9245\n",
      "Epoch 10/15\n",
      "657/657 - 5s - loss: 6.0060e-04 - accuracy: 0.9998 - val_loss: 0.6824 - val_accuracy: 0.9272\n",
      "Epoch 11/15\n",
      "657/657 - 5s - loss: 2.6965e-04 - accuracy: 0.9999 - val_loss: 0.6409 - val_accuracy: 0.9272\n",
      "Epoch 12/15\n",
      "657/657 - 5s - loss: 9.5503e-04 - accuracy: 0.9998 - val_loss: 0.8009 - val_accuracy: 0.9204\n",
      "Epoch 13/15\n",
      "657/657 - 4s - loss: 6.7908e-04 - accuracy: 0.9997 - val_loss: 0.7554 - val_accuracy: 0.9197\n",
      "Epoch 14/15\n",
      "657/657 - 5s - loss: 1.6784e-04 - accuracy: 1.0000 - val_loss: 0.6009 - val_accuracy: 0.9347\n",
      "Epoch 15/15\n",
      "657/657 - 5s - loss: 5.5703e-05 - accuracy: 1.0000 - val_loss: 0.7072 - val_accuracy: 0.9252\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6839 - accuracy: 0.9265\n",
      "########################################################\n",
      "Iteración 1 de 5\n",
      "########################################################\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 48, 48, 32)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 46, 46, 32)        9248      \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 46, 46, 32)        67712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 21, 21, 64)        28224     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,019,175\n",
      "Trainable params: 1,018,919\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.9232 - accuracy: 0.6914 - val_loss: 0.6044 - val_accuracy: 0.8163\n",
      "Epoch 2/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.3162 - accuracy: 0.8961 - val_loss: 0.3953 - val_accuracy: 0.8803\n",
      "Epoch 3/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.1521 - accuracy: 0.9497 - val_loss: 0.2651 - val_accuracy: 0.9163\n",
      "Epoch 4/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0725 - accuracy: 0.9779 - val_loss: 0.2711 - val_accuracy: 0.9136\n",
      "Epoch 5/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.3164 - val_accuracy: 0.9075\n",
      "Epoch 6/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.3995 - val_accuracy: 0.8871\n",
      "Epoch 7/15\n",
      "657/657 [==============================] - 5s 7ms/step - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.3930 - val_accuracy: 0.9163\n",
      "Epoch 8/15\n",
      "657/657 [==============================] - 5s 7ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.3175 - val_accuracy: 0.9340\n",
      "Epoch 9/15\n",
      "657/657 [==============================] - 4s 7ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.3575 - val_accuracy: 0.9286\n",
      "Epoch 10/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.3399 - val_accuracy: 0.9279\n",
      "Epoch 11/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.2853 - val_accuracy: 0.9327\n",
      "Epoch 12/15\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.4249 - val_accuracy: 0.9347\n",
      "Epoch 13/15\n",
      "657/657 [==============================] - 7s 10ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.3680 - val_accuracy: 0.9354\n",
      "Epoch 14/15\n",
      "657/657 [==============================] - 9s 13ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.4380 - val_accuracy: 0.9218\n",
      "Epoch 15/15\n",
      " 90/657 [===>..........................] - ETA: 6s - loss: 0.0031 - accuracy: 0.9993"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_1(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res11,res12,evaluations,evaluations1 = run_experiment(2,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos los experimentos\n",
    "\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_2(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res21,res22,evaluations,evaluations2 = run_experiment(2,EPOCHS)\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_3(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res31,res32,evaluations,evaluations3 = run_experiment(2,EPOCHS)\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_4(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res41,res42,evaluations,evaluations4 = run_experiment(2,EPOCHS)\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_5(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res51,res52,evaluations,evaluations5 = run_experiment(2,EPOCHS)\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_6(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res61,res62,evaluations,evaluations6 = run_experiment(2,EPOCHS)\n",
    "\n",
    "BATCH_SIZE,EPOCHS,opt = set_hiper_to_exp_7(BATCH_SIZE,EPOCHS,LEARNING_RATE)\n",
    "res71,res72,evaluations,evaluations7 = run_experiment(2,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(nn_base_arch,epochs,checkpoint,dense = False):\n",
    "    nn = select_network(nn_base_arch)\n",
    "    \n",
    "    if dense == True :\n",
    "        model = build_dense(nn)\n",
    "    else:\n",
    "        model = build(nn)\n",
    "        \n",
    "    cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,epochs=EPOCHS,callbacks=[cpoint],batch_size = BATCH_SIZE,verbose=0)\n",
    "      \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation\n",
    "\n",
    "def run_train_w_model(model,epochs,dense = False):\n",
    "        \n",
    "    #cpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,validation_data=(x_validation, y_validation),\n",
    "                        epochs=EPOCHS,batch_size = BATCH_SIZE)\n",
    "    \n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "        \n",
    "    return history,evaluation,model\n",
    "\n",
    "def re_train(model,epocas):\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=0)\n",
    "    history = model.fit(x_train, y_train,validation_data=(x_validation, y_validation),epochs=EPOCHS,batch_size = BATCH_SIZE,verbose=2)\n",
    "    evaluation = model.evaluate(x_test, y_test)\n",
    "    return history,evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(index_model,epochs,iterations = ITERATIONS_PER_EXP):\n",
    "    result = []\n",
    "    result_post_tf = []\n",
    "    evaluations = []\n",
    "    evaluations_post_tf = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        #checkpoint =\"../TFG/Modelos/balanced_model_\"+nn_base_arch+\"_exp4_v_\"+str(i)+\"_EXP0.h5\"\n",
    "        \n",
    "        if index_model == 0:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = False, withDropout=True)\n",
    "        if index_model == 1:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = True, withDropout=True)\n",
    "        if index_model == 2:\n",
    "            model = full_build_cnn_soco(withBatchNormalization = True, withDropout=False)\n",
    "        \n",
    "        h,e,tf_model = run_train_w_model(model,epochs)\n",
    "        result.append(h)\n",
    "        evaluations.append(e)\n",
    "        \n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "        layers = tf_model.layers[0:-1]\n",
    "        for layer in layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        h_retrain,e_retrain = re_train(tf_model,epochs)\n",
    "        \n",
    "        result_post_tf.append(h_retrain)\n",
    "        evaluations_post_tf.append(e_retrain)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"########################################################\")\n",
    "        print(\"Iteración \"+str(i+1) +\" de \"+ str(iterations))\n",
    "        print(\"########################################################\")\n",
    "        \n",
    "    return result,result_post_tf,evaluations,evaluations_post_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de representación gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representamos gráficamente los resultados obtenidos al experimentar con el tamaño del lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values(resx1,resx2,i,val):\n",
    "    res=[]\n",
    "    if val != \"loss\":\n",
    "        res.append(0.0)\n",
    "    for e in resx1[i].history[val]:\n",
    "        res.append(e)\n",
    "    for e in resx2[i].history[val]:\n",
    "        res.append(e)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_acc_all_experiments(res11,res12,res21,res22,res31,res32,res41,res42,res51,res52,res61,res62,res71,res72,epochs):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    acc = \"accuracy\"\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        precisiones = compute_values(res11,res12,i,acc)\n",
    "            \n",
    "        precisiones1 = compute_values(res21,res22,i,acc)\n",
    "        \n",
    "        precisiones2 = compute_values(res31,res32,i,acc)\n",
    "        \n",
    "        precisiones3 = compute_values(res41,res42,i,acc)\n",
    "        \n",
    "        precisiones4 = compute_values(res51,res52,i,acc)\n",
    "        \n",
    "        precisiones5 = compute_values(res61,res62,i,acc)\n",
    "        \n",
    "        precisiones6 = compute_values(res71,res72,i,acc)\n",
    "        \n",
    "        loss = compute_values(res11,res12,i,\"loss\")\n",
    "        \n",
    "        loss1 = compute_values(res21,res22,i,\"loss\")\n",
    "        \n",
    "        loss2 = compute_values(res31,res32,i,\"loss\")\n",
    "        \n",
    "        loss3 = compute_values(res41,res42,i,\"loss\")\n",
    "        \n",
    "        loss4 = compute_values(res51,res52,i,\"loss\")\n",
    "        \n",
    "        loss5 = compute_values(res61,res62,i,\"loss\")\n",
    "        \n",
    "        loss6 = compute_values(res71,res72,i,\"loss\")\n",
    "\n",
    "    \n",
    "        if i == 0:\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1], label=\"RMSProp\",color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"SGD\",color='yellow')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1], label=\"Adam\",color='red')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1], label=\"Amsgrad\",color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), loss[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss1[0:epochs+1], label=\"SGD\",color='yellow')\n",
    "            plt.plot(np.arange(0, epochs+1), loss2[0:epochs+1],color='red')\n",
    "            plt.plot(np.arange(0, epochs+1), loss3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), precisiones[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones1[0:epochs+1], label=\"SGD\",color='yellow')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones2[0:epochs+1],color='red')\n",
    "            plt.plot(np.arange(0, epochs+1), precisiones3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones4[0:epochs+1], label=\"Adamax\",color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones5[0:epochs+1], label=\"Nadam\",color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), precisiones6[0:epochs+1], label=\"Adadelta\",color='yellowgreen')\n",
    "            \n",
    "            plt.plot(np.arange(0, epochs+1), loss[0:epochs+1],color='green')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss1[0:epochs+1],color='yellow')\n",
    "            plt.plot(np.arange(0, epochs+1), loss2[0:epochs+1],color='red')\n",
    "            plt.plot(np.arange(0, epochs+1), loss3[0:epochs+1],color='aqua')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss4[0:epochs+1],color='red')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss5[0:epochs+1],color='deeppink')\n",
    "            #plt.plot(np.arange(0, epochs+1), loss6[0:epochs+1],color='yellowgreen')\n",
    "            \n",
    "        del precisiones\n",
    "        del precisiones1\n",
    "        del precisiones2\n",
    "        del precisiones3\n",
    "        del precisiones4\n",
    "        del precisiones5\n",
    "        del precisiones6\n",
    "\n",
    "        \n",
    "    plt.title(\"Training Accuracy and Test Results\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Exp_2_Mix_Results.jpg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_all_experiments(res11,res12,res21,res22,res31,res32,res41,res42,res51,res52,res61,res62,res71,res72,29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos marcos de datos para analizar los resultados de evaluar los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision1 = []\n",
    "precision2 = []\n",
    "precision3 = []\n",
    "precision4 = []\n",
    "precision5 = []\n",
    "precision6 = []\n",
    "precision7 = []\n",
    "\n",
    "for e in evaluations1:\n",
    "    element = e[1]\n",
    "    precision1.append(element)\n",
    "\n",
    "for e in evaluations2:\n",
    "    element = e[1]\n",
    "    precision2.append(element)\n",
    "    \n",
    "for e in evaluations3:\n",
    "    element = e[1]\n",
    "    precision3.append(element)\n",
    "\n",
    "for e in evaluations4:\n",
    "    element = e[1]\n",
    "    precision4.append(element)\n",
    "    \n",
    "for e in evaluations5:\n",
    "    element = e[1]\n",
    "    precision5.append(element)\n",
    "\n",
    "for e in evaluations6:\n",
    "    element = e[1]\n",
    "    precision6.append(element)\n",
    "    \n",
    "for e in evaluations7:\n",
    "    element = e[1]\n",
    "    precision7.append(element)\n",
    "\n",
    "  \n",
    "    \n",
    "d = {'RMSProp': precision1,'SGD': precision2,'Adam': precision3,\n",
    "     'Amsgrad': precision4,'Adamax': precision5,'Nadam': precision6,'Adadelta': precision7}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "print(df.mean())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vgg16vs19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
